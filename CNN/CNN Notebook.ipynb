{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, Convolutional Neural Networks (CNNs) are used for working with image data.\n",
    "This includes image classification, image detection, image captioning, and other similar tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a convolutional network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN is a neural network that consists of layers of fundamentally three main types: <br>\n",
    "-Convolutional layers <br>\n",
    "-Pooling layers <br>\n",
    "-Fully connected layers <br>\n",
    "\n",
    "We will go through each one of them separately. <br>\n",
    "\n",
    "Example of a CNN: <br>\n",
    "![convolutional network](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we work with images in PyTorch (and in general, data processing overall) we express them as arrays of dimensions corresponding to length, width, and number of channels; with each array element corresponding to a pixel value of the image. <br>\n",
    "During pre-processing, we convert the input images into tensors so that the model can work with the data.\n",
    "In this process, we implement transformations, which are specific operations applied to input images to make them easier to work with. <br> Examples of transformations include converting to tensor, and resizing to a fixed size for each image (since models don't work with input images of varying size), as well as normalization (this will be described later). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the usual first step in a CNN is to pass the inputs through a convolutional layer.<br>\n",
    "A convolutional layer consists of an array called a filter, which is passed over the input array like a scanner, covering every pixel at least once, and computing a calculation called a convolution.<br>The filter has parameters, typically kernel size (which measures the dimensions of the filter), stride (which is the number of pixels the filter moves after each convolution is done) and number of output channels.<br><br>\n",
    "Here are mathematical examples of the convolution operation: <br>\n",
    "![Image of Conv operation](https://miro.medium.com/max/928/0*e-SMFTzO8r7skkpc) <br><br>\n",
    "![Image2 of conv](https://tutorials.retopall.com/wp-content/uploads/2019/03/ConvOperation-1024x622.png)<br><br>\n",
    "Sometimes we add padding to the input image to change the dimensions of the output image. The input image is bordered by pixels containing zero, in order to change the dimensions without changing the values.<br><br>\n",
    "![Padding](http://media5.datahacker.rs/2018/11/sl_1.png)<br><br>\n",
    "There are common types of padding that are used: valid padding is when there is no padding, and same padding is when the input size and output size are the same.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the guiding principle of convolutional layers is the hierarchial structure of images. What this means, is that images are made up of various building blocks that grow in complexity as we climb the hierarchial structure of elements. <br>\n",
    "For example, the most basic of elements would be things such as lines, edges, vertices, etc. The next hierarchy might include more complicated shapes like circles, squares, etc. and the complexity increases as we move on. <br>\n",
    "![image hierarchy](https://miro.medium.com/max/3820/1*fLGuAUT5imTIGAeA4zzaWA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can conclude that the main function of a convolutional layer is to detect these specific elements present in the image. The filter has learnable parameters and eventually, each filter will end up recognizing a specific element present in the input image (as shown above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next type of layer we see here is a pooling layer. Here, the \"filter\" will perform an operation (usually maximum or average) on the elements present in the filter's region and they condense the image to a smaller working size (retaining the values deemed important for detection and removing the ones that are not).<br>\n",
    "Pooling layers are convenient as they do not have any learnable parameters as such. The arguments for a pooling layer are generally kernel size and stride (meanings same as in convolutional layers). Common pooling layers include maxpool and avgpool.<br><br>\n",
    "![maxpool](https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png)\n",
    "![avgpool](https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a normal linear neural network that is present at the end of the convolutional and pooling layers. <br>\n",
    "At the end of the image processing model, the resulting image array is flattened into a one dimensional vector and this vector is passed into a fully connected neural network. Based on the task the model is being built to achieve, the resulting output is accordingly processed.<br>\n",
    "![fullyconnected](https://www.researchgate.net/profile/Arvind-Sreenivas/publication/343263135/figure/fig3/AS:918277995905024@1595945943003/Fully-connected-layer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a neural network, often we find that the input features are not of similar numerical values (for example, one feature may range from 0-1 while another feature can range from 0-100) and so this reduces the efficiency during training as the weights may similarly be differing in magnitude. <br>\n",
    "An issue that is cited is known as \"internal covariate shift\"; what this means, is as follows:<br>\n",
    "Usually, when we calculate backpropagation for a particular layer in a deep neural network, we estimate that the weights in the previous layers more or less do not change when we calculate gradients. However, this is not the case in reality and all the layers' weights are updated simultaneously in backprogation. The result of this is that the update procedure is endlessly chasing a moving target, so to speak. <br>\n",
    "To solve this issue, we implement a technique called batch normalisation (batchnorm for short). Here, the outputs of each layer (before or after activation, depending on the function) in a mini-batch are standardized by using their mean and standard deviation. This results in an increased training efficiency and also provides a regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receptive Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, a receptive field is the part of the input image that is visible to the convolutional filter at a given time. This receptive field increases linearly as the number of convolutional layers increases.<br>\n",
    "![receptivefield](https://qphs.fs.quoracdn.net/main-qimg-be1248566f3db6593d77a9ceba456314)<br>\n",
    "In the above image, the receptive field is the dark red portion with the lines attached.<br><br>\n",
    "\n",
    "We use the concept of receptive fields here since when dealing with images, we have a large number of input features and so it is impractical to connect all neurons to all neurons in the previous volume. Receptive fields help us focus on a particular part of the image at a time and hence decreases the \"clunkiness\" of having all neurons connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Convolutional Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are convenient for working with image data for a variety of reasons:<br>\n",
    "1. They allow for parameter sharing; i.e., a filter used to detect a particular element such as an edge in one part of the image will be similarly useful in another part of the image.\n",
    "2. Transitional invariance; basically, if the image is shifted a bit with respect to the original image, the network will not have any trouble identifying it as the original image.\n",
    "3. Lower number of parameters to be learned; here, the parameters for a convolutional layer are the elements of the filter used; pooling layers have no parameters to be learned at all.\n",
    "4. Transfer learning; let us go back to this image from the hierarchial representation section.<br> ![image hierarchy](https://miro.medium.com/max/3820/1*fLGuAUT5imTIGAeA4zzaWA.png)<br> As can be seen here, the lower level features of the different types of images are all the same. Therefore, it is reasonable to assume that the weights learned for these filters in one task can be carried over to be used in other tasks - and this assumption is proven correct. We sometimes use pretrained models for the lower levels of the hierarchy in images and only train the model for the problem-specific layers that will differ from case to case. This process is called transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some common CNN models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16](https://s3.ap-south-1.amazonaws.com/techleer/309.jpg)<br>\n",
    "![le-net](https://qjjnh3a9hpo1nukrg1fwoh71-wpengine.netdna-ssl.com/wp-content/uploads/2020/02/LeNet5_800px_web.jpg)<br><br>\n",
    "\n",
    "#### ResNet, a deep CNN which is made up of residual blocks depicted as shown (A is normal conv block, B is residual block):<br>\n",
    "![residual](https://lh3.googleusercontent.com/proxy/jRzDhaO_rrbzdTn7C_QEhtefCF1sY2GkDcthoKQ-dDh20NgkPhO2tG0bZBCYBmTsDmcif553YKHAcf1WOnijmSubOEKohaNlBH6GC5DgoM_J)<br>\n",
    "ResNet blocks are useful in deepening the neural network as they ensure that data from earlier layers is not completely lost in convolution processes and continues to be relevant throughout the model (as we take a sum of both unprocessed and processed inputs before creating the output of the residual block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
